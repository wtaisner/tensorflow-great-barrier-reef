{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "name": "SSD.ipynb",
   "provenance": [],
   "include_colab_link": true
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "6ca10803911a44daad15ed966b332a5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "VBoxView",
      "_dom_classes": [],
      "_model_name": "VBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_b114a058f3604b1d9e6a474aa6011727",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_975701566d354ecfa9b19bf1c5e0de5d",
       "IPY_MODEL_d578fe7be47c426398e49e3099d6dab1"
      ]
     }
    },
    "b114a058f3604b1d9e6a474aa6011727": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "975701566d354ecfa9b19bf1c5e0de5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "LabelView",
      "style": "IPY_MODEL_6aaacb8ce188499896660f91a2d35a98",
      "_dom_classes": [],
      "description": "",
      "_model_name": "LabelModel",
      "placeholder": "â€‹",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 0.91MB of 0.91MB uploaded (0.00MB deduped)\r",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_868497eb170e4f66b1fb031adafb3477"
     }
    },
    "d578fe7be47c426398e49e3099d6dab1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_2ee2f58f8635444eac737e6a030f2a9c",
      "_dom_classes": [],
      "description": "",
      "_model_name": "FloatProgressModel",
      "bar_style": "",
      "max": 1,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 1,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_1976e86740214a9890c78eaf2594db60"
     }
    },
    "6aaacb8ce188499896660f91a2d35a98": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "868497eb170e4f66b1fb031adafb3477": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "2ee2f58f8635444eac737e6a030f2a9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "1976e86740214a9890c78eaf2594db60": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "!wandb login"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_qievHg-wUI",
    "outputId": "921ea9fe-3a74-4854-b195-3c7abdd04755"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /home/witold/.netrc\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# try:\n",
    "#     import torchmetrics\n",
    "# except:\n",
    "#     !pip install torchmetrics\n",
    "#     import torchmetrics"
   ],
   "metadata": {
    "id": "eQVkm8cT7AzV"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "V6HmwJQ_Cmzv",
    "outputId": "b7bdc465-ced8-4064-db15-9052a56533d0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "import torchvision\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.detection.map import MeanAveragePrecision\n",
    "\n",
    "# this should probably be changed to something smart, right?\n",
    "KAGGLE_PATH_ANNOTATIONS = '/kaggle/input/tensorflow-great-barrier-reef/train.csv'\n",
    "KAGGLE_PATH_IMG_DIR = '/kaggle/input/tensorflow-great-barrier-reef/train_images/'\n",
    "LOCAL_PATH_ANNOTATIONS = 'data/train.csv'\n",
    "LOCAL_PATH_IMG_DIR = 'data/train_images/'\n",
    "COLAB_PATH_ANNOTATIONS = '/content/drive/MyDrive/data/train.csv'\n",
    "COLAB_PATH_IMG_DIR = '/content/drive/MyDrive/data/train_images/'\n",
    "\n",
    "wandb.config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 115,\n",
    "    \"batch_size\": 8,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"confidence_threshold\": 0.5,  # save a bounding box if model returned confidence above this threshold\n",
    "    \"optimizer\": 'SGD',\n",
    "}"
   ],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 1.961038,
     "end_time": "2022-01-14T11:54:15.753091",
     "exception": false,
     "start_time": "2022-01-14T11:54:13.792053",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2022-01-15T14:23:18.334933Z",
     "iopub.execute_input": "2022-01-15T14:23:18.335249Z",
     "iopub.status.idle": "2022-01-15T14:23:19.642124Z",
     "shell.execute_reply.started": "2022-01-15T14:23:18.335161Z",
     "shell.execute_reply": "2022-01-15T14:23:19.641445Z"
    },
    "trusted": true,
    "id": "jc9q8KQLAfkX"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class StarfishDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 annotations_file=LOCAL_PATH_ANNOTATIONS,\n",
    "                 img_dir=LOCAL_PATH_IMG_DIR\n",
    "                 ):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.annotated = self.img_labels[self.img_labels['annotations'] != '[]']  # get only annotated frames\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotated)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(os.path.join(self.img_dir, 'video_{}'.format(self.annotated.iloc[idx][0]),\n",
    "                                        '{}.jpg'.format(self.annotated.iloc[idx][2])))\n",
    "        min_image = image.min()\n",
    "        max_image = image.max()\n",
    "        # normalize image to 0-1 - required by torchvision\n",
    "        image -= min_image\n",
    "        image = torch.FloatTensor(image / max_image)\n",
    "        labels = self.annotated.iloc[idx][-1]\n",
    "        labels = ast.literal_eval(labels)\n",
    "        coords = []\n",
    "        for parsed_label in labels:\n",
    "            x1, y1 = parsed_label['x'], parsed_label['y']\n",
    "            x2, y2 = x1 + parsed_label['width'], y1 + parsed_label['height']\n",
    "            coords.append([x1, y1, x2, y2])\n",
    "\n",
    "        boxes = torch.FloatTensor(coords)\n",
    "        labels = torch.LongTensor([1 for _ in range(\n",
    "            len(coords))])  # label has to be integer, since we have only one label I coded it as 1 for simplicity\n",
    "        return image, boxes, labels"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-01-15T14:23:19.643849Z",
     "iopub.execute_input": "2022-01-15T14:23:19.644532Z",
     "iopub.status.idle": "2022-01-15T14:23:19.856879Z",
     "shell.execute_reply.started": "2022-01-15T14:23:19.644491Z",
     "shell.execute_reply": "2022-01-15T14:23:19.856050Z"
    },
    "trusted": true,
    "id": "tg35LhdxAfkd"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    targets = []\n",
    "    images = []\n",
    "    for imgs, boxes, labels in batch:\n",
    "        images.append(imgs)\n",
    "        d = {}\n",
    "        d['boxes'] = boxes\n",
    "        d['labels'] = labels\n",
    "        targets.append(d)\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "def slice_output(output: dict, confidence_threshold: float = wandb.config['confidence_threshold']) -> dict:\n",
    "    \"\"\"\n",
    "    this method is responsible for validating models output w.r.t confidence_threshold defined above.\n",
    "    It accepts an output dictionary from model, namely {'boxes':[], 'labels':[], 'scores':[]}\n",
    "    It returns a dictionary sliced to items with score above confidence_threshold\n",
    "    \"\"\"\n",
    "\n",
    "    num_valid_elements = np.sum(np.array(output['scores']) >= confidence_threshold)\n",
    "    # output['scores'] = output['scores'].to(device)\n",
    "    if num_valid_elements == 0:\n",
    "        num_valid_elements = 1\n",
    "    res = {}\n",
    "    for key, value in output.items():\n",
    "        res[key] = value[:num_valid_elements]\n",
    "    return res"
   ],
   "metadata": {
    "id": "UYRK09bPRfII"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# https://towardsdatascience.com/evaluating-performance-of-an-object-detection-model-137a349c517b\n",
    "\n",
    "# https://machinelearningmastery.com/fbeta-measure-for-machine-learning/\n",
    "\n",
    "def calc_iou(gt_bbox, pred_bbox):\n",
    "    \"\"\"\n",
    "    This function takes the predicted bounding box and ground truth bounding box and\n",
    "    return the IoU ratio\n",
    "    \"\"\"\n",
    "    x_topleft_gt, y_topleft_gt, x_bottomright_gt, y_bottomright_gt = gt_bbox\n",
    "    # print(gt_bbox, pred_bbox)\n",
    "    x_topleft_p, y_topleft_p, x_bottomright_p, y_bottomright_p = pred_bbox\n",
    "\n",
    "    if (x_topleft_gt > x_bottomright_gt) or (y_topleft_gt > y_bottomright_gt):\n",
    "        raise AssertionError(\"Ground Truth Bounding Box is not correct\")\n",
    "    if (x_topleft_p > x_bottomright_p) or (y_topleft_p > y_bottomright_p):\n",
    "        raise AssertionError(\"Predicted Bounding Box is not correct\", x_topleft_p, x_bottomright_p, y_topleft_p,\n",
    "                             y_bottomright_gt)\n",
    "\n",
    "    #if the GT bbox and predicted BBox do not overlap then iou=0\n",
    "    if x_bottomright_gt < x_topleft_p:\n",
    "        # If bottom right of x-coordinate  GT  bbox is less than or above the top left of x coordinate of  the predicted BBox\n",
    "\n",
    "        return 0.0\n",
    "    if (\n",
    "            y_bottomright_gt < y_topleft_p):  # If bottom right of y-coordinate  GT  bbox is less than or above the top left of y coordinate of  the predicted BBox\n",
    "\n",
    "        return 0.0\n",
    "    if (\n",
    "            x_topleft_gt > x_bottomright_p):  # If bottom right of x-coordinate  GT  bbox is greater than or below the bottom right  of x coordinate of  the predcited BBox\n",
    "\n",
    "        return 0.0\n",
    "    if (\n",
    "            y_topleft_gt > y_bottomright_p):  # If bottom right of y-coordinate  GT  bbox is greater than or below the bottom right  of y coordinate of  the predcited BBox\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "    GT_bbox_area = (x_bottomright_gt - x_topleft_gt + 1) * (y_bottomright_gt - y_topleft_gt + 1)\n",
    "    Pred_bbox_area = (x_bottomright_p - x_topleft_p + 1) * (y_bottomright_p - y_topleft_p + 1)\n",
    "\n",
    "    x_top_left = np.max([x_topleft_gt, x_topleft_p])\n",
    "    y_top_left = np.max([y_topleft_gt, y_topleft_p])\n",
    "    x_bottom_right = np.min([x_bottomright_gt, x_bottomright_p])\n",
    "    y_bottom_right = np.min([y_bottomright_gt, y_bottomright_p])\n",
    "\n",
    "    intersection_area = (x_bottom_right - x_top_left + 1) * (y_bottom_right - y_top_left + 1)\n",
    "\n",
    "    union_area = (GT_bbox_area + Pred_bbox_area - intersection_area)\n",
    "\n",
    "    return intersection_area / union_area\n",
    "\n",
    "\n",
    "def calc_precision_recall(image_results):\n",
    "    \"\"\"Calculates precision and recall from the set of images\n",
    "    Args:\n",
    "        image_results (dict): dictionary formatted like:\n",
    "            {\n",
    "                'img_id1': {'true_pos': int, 'false_pos': int, 'false_neg': int},\n",
    "                'img_id2': ...\n",
    "                ...\n",
    "            }\n",
    "    Returns:\n",
    "        tuple: of floats of (precision, recall)\n",
    "    \"\"\"\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    for img_id, res in image_results.items():\n",
    "        true_positive += res['true_positive']\n",
    "        false_positive += res['false_positive']\n",
    "        false_negative += res['false_negative']\n",
    "    try:\n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0.0\n",
    "    try:\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0.0\n",
    "    return (precision, recall)\n",
    "\n",
    "\n",
    "def get_single_image_results(gt_boxes, pred_boxes, iou_thr):\n",
    "    \"\"\"Calculates number of true_pos, false_pos, false_neg from single batch of boxes.\n",
    "    Args:\n",
    "        gt_boxes (list of list of floats): list of locations of ground truth\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        pred_boxes (dict): dict of dicts of 'boxes' (formatted like `gt_boxes`)\n",
    "            and 'scores'\n",
    "        iou_thr (float): value of IoU to consider as threshold for a\n",
    "            true prediction.\n",
    "    Returns:\n",
    "        dict: true positives (int), false positives (int), false negatives (int)\n",
    "    \"\"\"\n",
    "    all_pred_indices = range(len(pred_boxes))\n",
    "    all_gt_indices = range(len(gt_boxes))\n",
    "    if len(all_pred_indices) == 0:\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        return {'true_positive': tp, 'false_positive': fp, 'false_negative': fn}\n",
    "    if len(all_gt_indices) == 0:\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        return {'true_positive': tp, 'false_positive': fp, 'false_negative': fn}\n",
    "\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "\n",
    "    for ipb, pred_box in enumerate(pred_boxes):\n",
    "        for igb, gt_box in enumerate(gt_boxes):\n",
    "            iou = calc_iou(gt_box, pred_box)\n",
    "            if iou >= iou_thr:\n",
    "                TP += 1\n",
    "            elif 0.0 < iou < iou_thr:\n",
    "                FP += 1\n",
    "            elif iou == 0.0:\n",
    "                pass\n",
    "\n",
    "    if TP == 0 and FP == 0:\n",
    "        FN = len(gt_boxes)  # none of the groundtruth had been detected\n",
    "        return {'true_positive': 0, 'false_positive': 0, 'false_negative': FN}\n",
    "    else:\n",
    "        return {'true_positive': TP, 'false_positive': FP, 'false_negative': len(gt_boxes) - TP - FP}"
   ],
   "metadata": {
    "id": "MLqsyZ4m7Wjr"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(23)\n",
    "\n",
    "dataset = StarfishDataset()\n",
    "print(len(dataset))\n",
    "train_size = 3000\n",
    "test_size = (len(dataset) - train_size) // 2\n",
    "val_size = len(dataset) - train_size - test_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_size, val_size, test_size))\n",
    "\n",
    "print(\n",
    "    'Train dataset: {} instances, validation dataset: {}, test dataset: {}'.format(len(train_dataset), len(val_dataset),\n",
    "                                                                                   len(test_dataset)))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=wandb.config['batch_size'], shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=wandb.config['batch_size'], shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=wandb.config['batch_size'], shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "cpu = torch.device('cpu')\n",
    "print('Used device: {}'.format(device))\n",
    "\n",
    "\n",
    "num_classes = 2  # starfish and not starfish I guess    "
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-01-15T14:25:03.794536Z",
     "iopub.execute_input": "2022-01-15T14:25:03.794811Z",
     "iopub.status.idle": "2022-01-15T14:25:04.483121Z",
     "shell.execute_reply.started": "2022-01-15T14:25:03.794780Z",
     "shell.execute_reply": "2022-01-15T14:25:04.482306Z"
    },
    "trusted": true,
    "id": "wkGu_Sh6Afkh",
    "outputId": "43af2b85-b447-4ccc-c49c-f5a1a51493bc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4919\n",
      "Train dataset: 3000 instances, validation dataset: 960, test dataset: 959\n",
      "Used device: cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.init(project=\"great-barrier-reef\", entity=\"ap-wt\", config=wandb.config)\n",
    "\n",
    "model = torchvision.models.detection.ssd300_vgg16(pretrained=False, pretrained_backbone=False, num_classes=num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "if wandb.config['optimizer'] == 'SGD':\n",
    "    optimizer = torch.optim.SGD(params, lr=wandb.config['learning_rate'], momentum=wandb.config['momentum'],\n",
    "                                weight_decay=wandb.config['weight_decay'])\n",
    "elif wandb.config['optimizer'] == 'Adam':\n",
    "    optimizer = torch.optim.Adam(params, lr=wandb.config['learning_rate'], weight_decay=wandb.config['weight_decay'])\n",
    "elif wandb.config['optimizer'] == 'AdamW':\n",
    "    optimizer = torch.optim.AdamW(params, lr=wandb.config['learning_rate'], weight_decay=wandb.config['weight_decay'])\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261,
     "referenced_widgets": [
      "6ca10803911a44daad15ed966b332a5e",
      "b114a058f3604b1d9e6a474aa6011727",
      "975701566d354ecfa9b19bf1c5e0de5d",
      "d578fe7be47c426398e49e3099d6dab1",
      "6aaacb8ce188499896660f91a2d35a98",
      "868497eb170e4f66b1fb031adafb3477",
      "2ee2f58f8635444eac737e6a030f2a9c",
      "1976e86740214a9890c78eaf2594db60"
     ]
    },
    "id": "zJwbEElZ2gdt",
    "outputId": "ecbb6c40-8b0d-4322-8cbe-31391a877585"
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33map-wt\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/ap-wt/great-barrier-reef/runs/2rxdhpe1\" target=\"_blank\">glamorous-sun-129</a></strong> to <a href=\"https://wandb.ai/ap-wt/great-barrier-reef\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# https://pytorch.org/vision/stable/models.html#runtime-characteristics see Faster R-CNN for the details of this model, what it requires, returns, etc\n",
    "\n",
    "# https://github.com/pytorch/vision/blob/main/references/detection/engine.py probably see training and eval loops here\n",
    "\n",
    "\n",
    "wandb.watch(model, log=\"all\", log_freq=50)\n",
    "for e in tqdm(range(wandb.config['epochs'])):\n",
    "    print('\\n')\n",
    "    model.train()\n",
    "\n",
    "    for idx, (images, targets) in enumerate(train_dataloader):\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "\n",
    "        for d in targets:\n",
    "            d['boxes'] = d['boxes'].to(device)\n",
    "            d['labels'] = d['labels'].to(device)\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 50 == 0:\n",
    "            wandb.log({\"epoch\": e, \"train_loss\": loss})\n",
    "\n",
    "        if device == torch.device(\"cuda\"):\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gt_boxes = dict()\n",
    "        pred_boxes = dict()\n",
    "        for idx, (images, targets) in enumerate(val_dataloader):\n",
    "\n",
    "            # for d in targets:\n",
    "            #     d['boxes'] = d['boxes'].to(device)\n",
    "            #     d['labels'] = d['labels'].to(device)\n",
    "            images = list(image.to(device) for image in images)\n",
    "            predictions = model(images)\n",
    "            outputs = [{k: v.to(cpu) for k, v in t.items()} for t in predictions]\n",
    "\n",
    "            outputs = [slice_output(out) for out in outputs]\n",
    "\n",
    "            metric.update(outputs, targets)\n",
    "            metrics = metric.compute()\n",
    "\n",
    "            gt_boxes[idx] = [d['boxes'].tolist()[0] for d in targets]\n",
    "            tmp_pred_boxes = {'boxes': [], \"scores\": []}\n",
    "            for d in outputs:\n",
    "                try:\n",
    "                    tmp_pred_boxes['boxes'].append(d['boxes'].tolist()[0])\n",
    "                    tmp_pred_boxes['scores'].append(d['scores'].tolist()[0])\n",
    "                except:\n",
    "                    print('fail')\n",
    "                    continue\n",
    "            pred_boxes[idx] = tmp_pred_boxes\n",
    "\n",
    "        tmp = {}\n",
    "        for idx, (gt, prd) in enumerate(zip(gt_boxes.values(), pred_boxes.values())):\n",
    "            res = get_single_image_results(gt, prd['boxes'], 0.5)\n",
    "            tmp[idx] = res\n",
    "        precision, recall = calc_precision_recall(tmp)\n",
    "        try:\n",
    "            F2 = (5 * precision * recall) / (4 * precision + recall)\n",
    "        except:\n",
    "            F2 = 0\n",
    "\n",
    "        wandb.log({'eval/MAP': metrics['map'],\n",
    "                   'eval/MAR_1': metrics['mar_1'],\n",
    "                   'eval/Precision': precision,\n",
    "                   'eval/Recall': recall,\n",
    "                   'eval/F2 score': F2})\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-01-15T14:25:07.542207Z",
     "iopub.execute_input": "2022-01-15T14:25:07.542717Z",
     "iopub.status.idle": "2022-01-15T14:27:34.233257Z",
     "shell.execute_reply.started": "2022-01-15T14:25:07.542683Z",
     "shell.execute_reply": "2022-01-15T14:27:34.232060Z"
    },
    "trusted": true,
    "id": "IZq9ZxqRAfkn",
    "outputId": "82539108-dfc1-4b36-b070-d5ddfba4c58d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/115 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/witold/Studia/Semestr-5/tensorflow-great-barrier-reef/venv/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "metric = MeanAveragePrecision()\n",
    "with torch.no_grad():\n",
    "    gt_boxes = dict()\n",
    "    pred_boxes = dict()\n",
    "    for idx, (images, targets) in enumerate(val_dataloader):\n",
    "\n",
    "        # for d in targets:\n",
    "        #     d['boxes'] = d['boxes'].to(device)\n",
    "        #     d['labels'] = d['labels'].to(device)\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        predictions = model(images)\n",
    "        outputs = [{k: v.to(cpu) for k, v in t.items()} for t in predictions]\n",
    "\n",
    "        outputs = [slice_output(out) for out in outputs]\n",
    "\n",
    "        metric.update(outputs, targets)\n",
    "        metrics = metric.compute()\n",
    "\n",
    "        gt_boxes[idx] = [d['boxes'].tolist()[0] for d in targets]\n",
    "        tmp_pred_boxes = {'boxes': [], \"scores\": []}\n",
    "        for d in outputs:\n",
    "            try:\n",
    "                tmp_pred_boxes['boxes'].append(d['boxes'].tolist()[0])\n",
    "                tmp_pred_boxes['scores'].append(d['scores'].tolist()[0])\n",
    "            except:\n",
    "                print('fail')\n",
    "                continue\n",
    "        pred_boxes[idx] = tmp_pred_boxes\n",
    "\n",
    "    tmp = {}\n",
    "    for idx, (gt, prd) in enumerate(zip(gt_boxes.values(), pred_boxes.values())):\n",
    "        res = get_single_image_results(gt, prd['boxes'], 0.5)\n",
    "        tmp[idx] = res\n",
    "    precision, recall = calc_precision_recall(tmp)\n",
    "    try:\n",
    "        F2 = (5 * precision * recall) / (4 * precision + recall)\n",
    "    except:\n",
    "        F2 = 0\n",
    "\n",
    "    columns = ['metric', 'test/value']\n",
    "    test_metrics = [\n",
    "        ['MAP', metrics['map']],\n",
    "        ['MAR1', metrics['mar_1']],\n",
    "        ['Precision', precision],\n",
    "        ['Recall', recall],\n",
    "        ['F2 score', F2]\n",
    "    ]\n",
    "    table = wandb.Table(columns=columns, data=test_metrics, allow_mixed_types=True)\n",
    "    wandb.log({\"Test set metrics\": table})\n",
    "    wandb.log({'test/MAP': metrics['map'],\n",
    "           'test/MAR_1': metrics['mar_1'],\n",
    "           'test/Precision': precision,\n",
    "           'test/Recall': recall,\n",
    "           'test/F2 score': F2})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'FasterRCNN-from-scratch.pt')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "5cwEVYTrq3Wf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "19a09830-3f3c-4e4a-9bac-378d721302c3"
   }
  }
 ]
}