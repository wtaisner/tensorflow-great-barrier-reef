{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import ast\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "KAGGLE_PATH_ANNOTATIONS = '/kaggle/input/tensorflow-great-barrier-reef/train.csv'\n",
    "KAGGLE_PATH_IMG_DIR = '/kaggle/input/tensorflow-great-barrier-reef/train_images/'\n",
    "LOCAL_PATH_ANNOTATIONS = 'data/train.csv'\n",
    "LOCAL_PATH_IMG_DIR = 'data/train_images/'\n",
    "\n",
    "\n",
    "# TODO: pewnie można zrobić zmienne globalne z directory path, żeby podmieniać na kagglową jak puszczamy w kagglu i na własną, jak puszczamy lokalnie\n",
    "# *ewentualnie lokalnie ustawić jak w kaggle xd"
   ],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 1.961038,
     "end_time": "2022-01-14T11:54:15.753091",
     "exception": false,
     "start_time": "2022-01-14T11:54:13.792053",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2022-01-15T14:23:18.334933Z",
     "iopub.execute_input": "2022-01-15T14:23:18.335249Z",
     "iopub.status.idle": "2022-01-15T14:23:19.642124Z",
     "shell.execute_reply.started": "2022-01-15T14:23:18.335161Z",
     "shell.execute_reply": "2022-01-15T14:23:19.641445Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class StarfishDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 annotations_file=LOCAL_PATH_ANNOTATIONS,\n",
    "                 img_dir=LOCAL_PATH_IMG_DIR\n",
    "                 ):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.annotated = self.img_labels[self.img_labels['annotations'] != '[]']  # get only annotated frames\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotated)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(os.path.join(self.img_dir, 'video_{}'.format(self.annotated.iloc[idx][0]),\n",
    "                                        '{}.jpg'.format(self.annotated.iloc[idx][2])))\n",
    "        min_image = image.min()\n",
    "        max_image = image.max()\n",
    "        # normalize image to 0-1 - required by torchvision\n",
    "        image -= min_image\n",
    "        image = torch.FloatTensor(image/max_image)\n",
    "\n",
    "        labels = self.annotated.iloc[idx][-1]\n",
    "        labels = ast.literal_eval(labels)\n",
    "        coords = []\n",
    "        for parsed_label in labels:\n",
    "            x1, y1 = parsed_label['x'], parsed_label['y']\n",
    "            x2, y2 = x1+parsed_label['width'], y1+parsed_label['height']\n",
    "            coords.append([x1, y1, x2, y2])\n",
    "            \n",
    "        target = [torch.FloatTensor(coords), torch.LongTensor([0 for _ in range(len(coords))])] # label has to be integer, since we have only one label I coded it as 1 for simplicity\n",
    "\n",
    "        return image, target\n",
    "\n",
    "dataset = StarfishDataset()\n",
    "dataset.__getitem__(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-01-15T14:23:19.643849Z",
     "iopub.execute_input": "2022-01-15T14:23:19.644532Z",
     "iopub.status.idle": "2022-01-15T14:23:19.856879Z",
     "shell.execute_reply.started": "2022-01-15T14:23:19.644491Z",
     "shell.execute_reply": "2022-01-15T14:23:19.856050Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[0.1451, 0.1020, 0.0980,  ..., 0.0000, 0.0000, 0.0000],\n          [0.1686, 0.1255, 0.1059,  ..., 0.0000, 0.0000, 0.0000],\n          [0.1373, 0.1176, 0.1098,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.2706, 0.3294, 0.2275,  ..., 0.0627, 0.0627, 0.0627],\n          [0.2588, 0.2549, 0.2706,  ..., 0.1137, 0.1216, 0.0980],\n          [0.2353, 0.2745, 0.3020,  ..., 0.1373, 0.1804, 0.2039]],\n \n         [[0.5686, 0.5569, 0.5686,  ..., 0.5922, 0.5922, 0.5882],\n          [0.5961, 0.5804, 0.5765,  ..., 0.5922, 0.5922, 0.5882],\n          [0.5647, 0.5725, 0.5843,  ..., 0.5922, 0.5922, 0.5882],\n          ...,\n          [0.6078, 0.7686, 0.8000,  ..., 0.5059, 0.5137, 0.5255],\n          [0.6118, 0.6275, 0.7137,  ..., 0.5373, 0.5608, 0.5451],\n          [0.5961, 0.6510, 0.7098,  ..., 0.5608, 0.6275, 0.6706]],\n \n         [[0.8118, 0.7882, 0.8039,  ..., 0.9961, 0.9961, 0.9922],\n          [0.8275, 0.8118, 0.8118,  ..., 0.9961, 0.9961, 0.9922],\n          [0.7882, 0.7961, 0.8078,  ..., 0.9961, 0.9961, 0.9922],\n          ...,\n          [0.9333, 1.0000, 0.9922,  ..., 0.7333, 0.7255, 0.7137],\n          [0.9490, 0.9412, 0.9804,  ..., 0.7176, 0.7059, 0.6784],\n          [0.9373, 0.9804, 0.9961,  ..., 0.7020, 0.7294, 0.7569]]]),\n [tensor([[559., 213., 609., 245.]]), tensor([0])])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1)\n",
    "dataset = StarfishDataset()\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "print('Train dataset: {} instances, test dataset: {}'.format(len(train_dataset), len(test_dataset)))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = torch.device('cpu') # <----------------------------- manual switch to CPU, my GPU is too weak :(\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "num_classes = 1  # starfish and not starfish I guess\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# criterion = gio()\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-01-15T14:25:03.794536Z",
     "iopub.execute_input": "2022-01-15T14:25:03.794811Z",
     "iopub.status.idle": "2022-01-15T14:25:04.483121Z",
     "shell.execute_reply.started": "2022-01-15T14:25:03.794780Z",
     "shell.execute_reply": "2022-01-15T14:25:04.482306Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 3935 instances, test dataset: 984\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: finish eval loop\n",
    "# https://pytorch.org/vision/stable/models.html#runtime-characteristics see Faster R-CNN for the details of this model, what it requires, returns, etc\n",
    "for e in tqdm(range(1)):\n",
    "    model.train()\n",
    "    for images, targets in tqdm(train_dataloader):\n",
    "        # print('org images', images, 'org targets', targets)\n",
    "        target = []\n",
    "        for i in range(len(images)):\n",
    "            d = {}\n",
    "            d['boxes'] = targets[0][i].to(device)\n",
    "#             print(d['boxes'])\n",
    "            d['labels'] = targets[1][i].to(device)\n",
    "            target.append(d)\n",
    "        # for t in target:\n",
    "        #     print(t['boxes'])\n",
    "        #     print('--------')\n",
    "\n",
    "        images = images.to(device)\n",
    "\n",
    "        loss_dict = model(images, target)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    print('Reached eval')\n",
    "    with torch.no_grad():\n",
    "        groundtruth, predictions = None, None\n",
    "        for images, targets in tqdm(test_dataloader):\n",
    "            torch.cuda.empty_cache()\n",
    "            predictions = model(images)\n",
    "            print(predictions)\n",
    "\n",
    "    optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-01-15T14:25:07.542207Z",
     "iopub.execute_input": "2022-01-15T14:25:07.542717Z",
     "iopub.status.idle": "2022-01-15T14:27:34.233257Z",
     "shell.execute_reply.started": "2022-01-15T14:25:07.542683Z",
     "shell.execute_reply": "2022-01-15T14:27:34.232060Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/3935 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 1/3935 [00:25<28:19:16, 25.92s/it]\u001B[A\n",
      "  0%|          | 2/3935 [00:51<28:19:53, 25.93s/it]\u001B[A\n",
      "  0%|          | 3/3935 [01:17<28:19:50, 25.94s/it]\u001B[A\n",
      "  0%|          | 4/3935 [01:41<27:16:42, 24.98s/it]\u001B[A\n",
      "  0%|          | 5/3935 [02:05<26:48:34, 24.56s/it]\u001B[A\n",
      "  0%|          | 6/3935 [02:30<26:59:10, 24.73s/it]\u001B[A\n",
      "  0%|          | 7/3935 [02:54<26:43:52, 24.50s/it]\u001B[A\n",
      "  0%|          | 8/3935 [03:17<26:25:09, 24.22s/it]\u001B[A\n",
      "  0%|          | 9/3935 [03:42<26:42:13, 24.49s/it]\u001B[A"
     ]
    }
   ]
  }
 ]
}