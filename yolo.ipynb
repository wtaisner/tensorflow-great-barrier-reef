{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "try_yolo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMsAQbQPdUHRyiQAFDeZuvk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wtaisner/tensorflow-great-barrier-reef/blob/main/yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import wandb\n",
        "except:\n",
        "    !pip install wandb\n",
        "    import wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_HYC6V-LPca",
        "outputId": "c50fa4a9-ae6a-4bb1-942d-702f2206e51a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mannprz\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O3r_qxZwLGdj"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import torchmetrics\n",
        "except:\n",
        "    !pip install torchmetrics\n",
        "    import torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dSDzE_vLK0o",
        "outputId": "49819abb-c54f-473f-bca0-137d8dea2ee9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.io import read_image\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import torchvision\n",
        "import ast\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from torchmetrics.detection.map import MeanAveragePrecision\n",
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "\n",
        "# this should probably be changed to something smart, right?\n",
        "KAGGLE_PATH_ANNOTATIONS = '/kaggle/input/tensorflow-great-barrier-reef/train.csv'\n",
        "KAGGLE_PATH_IMG_DIR = '/kaggle/input/tensorflow-great-barrier-reef/train_images/'\n",
        "LOCAL_PATH_ANNOTATIONS = 'data/train.csv'\n",
        "LOCAL_PATH_IMG_DIR = 'data/train_images/'\n",
        "COLAB_PATH_ANNOTATIONS = '/content/drive/MyDrive/data/train.csv'\n",
        "COLAB_PATH_IMG_DIR = '/content/drive/MyDrive/data/train_images/'"
      ],
      "metadata": {
        "id": "yvhuJmtjLLdV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StarfishDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 annotations_file=COLAB_PATH_ANNOTATIONS,\n",
        "                 img_dir=COLAB_PATH_IMG_DIR\n",
        "                 ):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.annotated = self.img_labels[self.img_labels['annotations'] != '[]']  # get only annotated frames\n",
        "        self.img_dir = img_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotated)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = read_image(os.path.join(self.img_dir, 'video_{}'.format(self.annotated.iloc[idx][0]),\n",
        "                                        '{}.jpg'.format(self.annotated.iloc[idx][2])))\n",
        "        min_image = image.min()\n",
        "        max_image = image.max()\n",
        "        # normalize image to 0-1 - required by torchvision\n",
        "        image -= min_image\n",
        "        image = image/max_image\n",
        "        labels = self.annotated.iloc[idx][-1]\n",
        "        labels = ast.literal_eval(labels)\n",
        "        coords = []\n",
        "        for parsed_label in labels:\n",
        "            x, y = parsed_label['x'], parsed_label['y']\n",
        "            w, h = parsed_label['width'], parsed_label['height']\n",
        "            cx, cy = 0.5*w + x, 0.5*h + y\n",
        "            coords.append([0, cx/1280, cy/720, w/1280, h/720])\n",
        "\n",
        "        boxes = np.array(coords)\n",
        "        return image, boxes\n"
      ],
      "metadata": {
        "id": "G4v2hIs7LpOZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(23)\n",
        "\n",
        "# IF YOU WANT TO RUN PROPER MODEL LEARNING, MAKE SURE TO CHANGE DATASET SIZES\n",
        "\n",
        "dataset = StarfishDataset()\n",
        "print(len(dataset))\n",
        "train_size = 1000\n",
        "test_size = (len(dataset) - train_size)//2\n",
        "val_size = len(dataset) - train_size - test_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_size, val_size, test_size))\n",
        "\n",
        "# extract only small part of the data for faster learning / testing process\n",
        "# train_size = int(0.8 * len(train_dataset))\n",
        "# test_size = len(train_dataset) - train_size\n",
        "# train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
        "\n",
        "\n",
        "print('Train dataset: {} instances, validation dataset: {}, test dataset: {}'.format(len(train_dataset), len(val_dataset), len(test_dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy8PRH__Ogab",
        "outputId": "87709328-b5e0-471b-9017-3be416fa518e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4919\n",
            "Train dataset: 1000 instances, validation dataset: 1960, test dataset: 1959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset, path_img, path_lbl):\n",
        "  i = 0\n",
        "  for (image, label) in dataset:\n",
        "    file_image = path_img + '/im' + str(i) + '.jpg'\n",
        "    file_label = path_lbl + '/im' + str(i) + '.txt'\n",
        "    image = ToPILImage()(image)\n",
        "    image.save(file_image)\n",
        "    np.savetxt(file_label, label, fmt='%i %.4f %.4f %.4f %.4f')\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "W6nknaFGwFLS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd\n",
        "%cd /content/drive/MyDrive/data_yolo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkGGjLHQ79vQ",
        "outputId": "a107509a-e979-4316-cd43-a1bf3bcd5a0b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "/content/drive/MyDrive/data_yolo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path_img = 'images/train'\n",
        "train_path_lbl = 'labels/train'\n",
        "\n",
        "prepare_dataset(train_dataset, train_path_img, train_path_lbl)"
      ],
      "metadata": {
        "id": "5XHMRAiKz2ah"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_path_img = 'images/val'\n",
        "val_path_lbl = 'labels/val'\n",
        "\n",
        "prepare_dataset(val_dataset, val_path_img, val_path_lbl)"
      ],
      "metadata": {
        "id": "KsZXXxf68JNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path_img = 'images/test'\n",
        "test_path_lbl = 'labels/test'\n",
        "\n",
        "prepare_dataset(test_dataset, test_path_img, test_path_lbl)"
      ],
      "metadata": {
        "id": "HrhYeZh18JUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd\n",
        "%cd /content/drive/MyDrive\n",
        "%pwd"
      ],
      "metadata": {
        "id": "_L5fu44yHIpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5"
      ],
      "metadata": {
        "id": "0Ona_nYeIqNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 1280 --batch 32 --epochs 10 --data ../data_yolo.yaml --weights yolov5s.pt --cache --project \"great-barrier-reef\" --name \"randomrandom\" --entity \"ap-wt\""
      ],
      "metadata": {
        "id": "zVSkkmbsKwlf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}